{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV0MoiAYPOl6"
      },
      "source": [
        "# Instrument Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqyIksYOwFqK"
      },
      "source": [
        "## Downloading the python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_t-nwy5gaT0",
        "outputId": "99987a76-e7be-41ed-e126-4c56f41599e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (24.0)\n",
            "Requirement already satisfied: torch in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (2.1.2+cu121)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (2.1.2+cu121)\n",
            "Requirement already satisfied: filelock in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from torch) (2023.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from jinja2->torch) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Collecting torch-summary\n",
            "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: torch-summary\n",
            "Successfully installed torch-summary-1.4.5\n",
            "Requirement already satisfied: pandas in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (2.2.0)\n",
            "Requirement already satisfied: pyarrow in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (15.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from pandas) (1.26.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: librosa in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from librosa) (1.26.3)\n",
            "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from librosa) (1.12.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from librosa) (1.4.0)\n",
            "Requirement already satisfied: joblib>=0.14 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from librosa) (5.1.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from librosa) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from librosa) (4.9.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from librosa) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from pooch>=1.0->librosa) (4.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from pooch>=1.0->librosa) (23.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.11.17)\n",
            "Requirement already satisfied: tqdm in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from tqdm) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "# Download python packages required using pip\n",
        "!python -m pip install --upgrade pip\n",
        "!pip3 install torch torchaudio\n",
        "!pip install torch-summary\n",
        "!pip install pandas pyarrow\n",
        "!pip install librosa\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G2CbuOwcvdbO"
      },
      "outputs": [],
      "source": [
        "# Import python packages required\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "from torchsummary import summary\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "import numpy as np\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9btYITPrwalD"
      },
      "source": [
        "## Downloading the dataset\n",
        "\n",
        "The IRMAS dataset is downloaded to Google Drive and then unzipped into the local colab session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCEBq_BBi1uU",
        "outputId": "5e23ca31-2e7a-4997-d107-ed025cfa3dcc"
      },
      "outputs": [],
      "source": [
        "# Defining the path constants used\n",
        "AUDIO_FILES = 'wavfiles'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "950LTmC4w2yc"
      },
      "source": [
        "## Annotation File\n",
        "\n",
        "The code below creates an annotations file which provides the information for the dataset including the filenames and class labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "KKzgy03Yv4Ba",
        "outputId": "b758c5ae-5a39-4a0d-b806-470560dd4a70"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "      <th>ClassLabel</th>\n",
              "      <th>ClassID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0356dec7.wav</td>\n",
              "      <td>Acoustic_guitar</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0eeaebcb.wav</td>\n",
              "      <td>Acoustic_guitar</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10da16fb.wav</td>\n",
              "      <td>Acoustic_guitar</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>160c92d7.wav</td>\n",
              "      <td>Acoustic_guitar</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16c9a838.wav</td>\n",
              "      <td>Acoustic_guitar</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>d6144e01.wav</td>\n",
              "      <td>Violin_or_fiddle</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>d84c8c43.wav</td>\n",
              "      <td>Violin_or_fiddle</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>d914225a.wav</td>\n",
              "      <td>Violin_or_fiddle</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>e3f479f3.wav</td>\n",
              "      <td>Violin_or_fiddle</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>fec00143.wav</td>\n",
              "      <td>Violin_or_fiddle</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>300 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Filename        ClassLabel  ClassID\n",
              "0    0356dec7.wav   Acoustic_guitar        0\n",
              "1    0eeaebcb.wav   Acoustic_guitar        0\n",
              "2    10da16fb.wav   Acoustic_guitar        0\n",
              "3    160c92d7.wav   Acoustic_guitar        0\n",
              "4    16c9a838.wav   Acoustic_guitar        0\n",
              "..            ...               ...      ...\n",
              "295  d6144e01.wav  Violin_or_fiddle        9\n",
              "296  d84c8c43.wav  Violin_or_fiddle        9\n",
              "297  d914225a.wav  Violin_or_fiddle        9\n",
              "298  e3f479f3.wav  Violin_or_fiddle        9\n",
              "299  fec00143.wav  Violin_or_fiddle        9\n",
              "\n",
              "[300 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hfret\\AppData\\Local\\Temp\\ipykernel_11648\\2206804438.py:27: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  signal, fs = librosa.load(os.path.join(AUDIO_FILES, record), mono=True, sr=None)\n",
            "c:\\Users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\librosa\\core\\audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'wavfiles\\\\0356dec7.wav'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\librosa\\core\\audio.py:175\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\librosa\\core\\audio.py:208\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
            "File \u001b[1;32mc:\\Users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
            "\u001b[1;31mLibsndfileError\u001b[0m: Error opening 'wavfiles\\\\0356dec7.wav': System error.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m annotations\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFilename\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m annotations\u001b[38;5;241m.\u001b[39mindex:\n\u001b[1;32m---> 27\u001b[0m   signal, fs \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAUDIO_FILES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmono\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m   annotations\u001b[38;5;241m.\u001b[39mat[record, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLength\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mfs\n\u001b[0;32m     30\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(annotations\u001b[38;5;241m.\u001b[39mlabel))\n",
            "File \u001b[1;32mc:\\Users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\librosa\\core\\audio.py:183\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[0;32m    180\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    182\u001b[0m     )\n\u001b[1;32m--> 183\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "File \u001b[1;32mc:\\Users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
            "File \u001b[1;32mc:\\Users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\librosa\\util\\decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[0;32m     58\u001b[0m )\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\librosa\\core\\audio.py:239\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    236\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m    242\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
            "File \u001b[1;32mc:\\Users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\audioread\\__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\hfret\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\audioread\\rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wavfiles\\\\0356dec7.wav'"
          ]
        }
      ],
      "source": [
        "def create_file_annotations():\n",
        "  data = []\n",
        "\n",
        "  # Walking through the directories in the dataset\n",
        "  for (dirpath, dirnames, filenames) in os.walk(AUDIO_FILES):\n",
        "    for dir in dirnames:\n",
        "      for (dirpath, dirnames, filenames) in os.walk(os.path.join(AUDIO_FILES, dir)):\n",
        "        for filename in filenames:\n",
        "          data.append([filename, dir])\n",
        "    break\n",
        "\n",
        "  # Creating a pandas dataframe to hold this data\n",
        "  df = pd.DataFrame(data, columns =['Filename', 'ClassLabel'])\n",
        "\n",
        "  # Create class ID numbers from the class labels\n",
        "  df['ClassID'] = df.groupby(['ClassLabel']).ngroup()\n",
        "\n",
        "  return df\n",
        "\n",
        "annotations = create_file_annotations()\n",
        "\n",
        "# Display the dataframe\n",
        "display(annotations)\n",
        "\n",
        "annotations.set_index('Filename', inplace=True)\n",
        "for record in annotations.index:\n",
        "  signal, fs = librosa.load(os.path.join(AUDIO_FILES, record), mono=True, sr=None)\n",
        "  annotations.at[record, 'Length'] = signal.shape[0]/fs\n",
        "\n",
        "classes = list(np.unique(annotations.label))\n",
        "class_dist = annotations.groupby(['ClassLabel'])['Length'].mean()\n",
        "\n",
        "# Save the dataframe to a CSV file\n",
        "annotations.to_csv(path_or_buf=\"annotations.csv\", sep=',', encoding='utf-8', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aJlSnlKxaOJ"
      },
      "source": [
        "## Custom Dataset\n",
        "\n",
        "The custom dataset imports the audio dataset's annotation file into python and uses it to load the correct audio files with the corresponding class labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1iSbav-Eojr",
        "outputId": "3cb6597c-feea-46ce-df8b-84178d8a551e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device available: CUDA\n",
            "['flu' 'cel' 'org' 'cla' 'pia' 'sax' 'gac' 'gel' 'voi' 'tru' 'vio']\n"
          ]
        }
      ],
      "source": [
        "class AudioDataset(Dataset):\n",
        "  ''' Custom Dataset class for audio instrument classification '''\n",
        "\n",
        "  def __init__(self, annotation_file, audio_dir, device, transformation, transform_fs, num_samples):\n",
        "    '''\n",
        "    Parameters -\n",
        "        annotation_file: a path to a csv file with the annotations for the dataset\n",
        "                            these annotations should have the filename at index 0 and class ID at -1\n",
        "        audio_dir:       the path to the audio dataset directory\n",
        "        device:          the device in use (cude or cpu)\n",
        "        transformation:  provides the function for performing preprocessing on the data\n",
        "        transform_fs:    the samplerate to resample at\n",
        "        num_samples:     the length n of samples to cut / pad the data to\n",
        "    '''\n",
        "    # Read in the annotation file for the dataset\n",
        "    self.annotations = pd.read_csv(annotation_file)\n",
        "\n",
        "    # Defining attributes\n",
        "    self.audio_dir = audio_dir\n",
        "    self.device = device\n",
        "    self.transformation = transformation.to(self.device) # putting the data onto a cuda device is available\n",
        "    self.transform_fs = transform_fs\n",
        "    self.num_samples = num_samples\n",
        "\n",
        "    # Creating a unique ID for the classes\n",
        "    self.annotations.assign(id=self.annotations.groupby(['Instrument']).ngroup())\n",
        "\n",
        "  def __len__(self):\n",
        "    ''' Magic method to provide the length of the object '''\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    ''' Magic method to provide indexing for the object '''\n",
        "\n",
        "    # Gets the audio path and labelID for the index\n",
        "    audio_sample_path, labelID = self._get_audio_sample_path_and_label(index)\n",
        "\n",
        "    # Loads the audio input to the device\n",
        "    signal, fs = torchaudio.load(audio_sample_path, normalize=True)\n",
        "    signal = signal.to(self.device)\n",
        "\n",
        "    # Resamples and reshapes the audio\n",
        "    signal = self._resample_audio(signal, fs)\n",
        "    signal = self._reshape_audio(signal)\n",
        "\n",
        "    # Performs transformation on the device\n",
        "    signal = self.transformation(signal)\n",
        "\n",
        "    return signal, labelID\n",
        "\n",
        "  def get_class_labels(self):\n",
        "    ''' Public method to provide a list of the class labels '''\n",
        "    return self.annotations['Instrument'].unique()\n",
        "\n",
        "  def _get_audio_sample_path_and_label(self, index):\n",
        "      ''' Private get method for the sample path location\n",
        "                        and prediction labelID            '''\n",
        "      label = self.annotations.iloc[index, 2]\n",
        "      labelID = self.annotations.iloc[index, 7]\n",
        "      path = os.path.join(self.audio_dir, label, self.annotations.iloc[index, 0])\n",
        "      return path, labelID\n",
        "\n",
        "  def _resample_audio(self, signal, fs):\n",
        "      # Resample the audio signal if needed\n",
        "      if fs != self.transform_fs:\n",
        "          resampler = torchaudio.transforms.Resample(fs, self.transform_fs).to(self.device)\n",
        "          signal = resampler(signal)\n",
        "      return signal\n",
        "\n",
        "  def _reshape_audio(self, signal):\n",
        "      # Convert the signal to mono if needed\n",
        "      if signal.shape[0] > 1:\n",
        "          signal = torch.mean(signal, dim=0, keepdim=True)\n",
        "\n",
        "      # Cut the signal if needed\n",
        "      if signal.shape[1] > self.num_samples:\n",
        "          signal = signal[:, :self.num_samples]\n",
        "\n",
        "      # Pad the signal if needed\n",
        "      if signal.shape[1] < self.num_samples:\n",
        "          signal = torch.nn.functional.pad(signal, (0, self.num_samples - signal.shape[1]))\n",
        "\n",
        "      return signal\n",
        "\n",
        "# Defining the dataset and dataloader constants\n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Creating a mel spectogram for the feature transformation\n",
        "mel_spectogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    n_fft=1024,\n",
        "    hop_length=512,\n",
        "    n_mels=64\n",
        ")\n",
        "\n",
        "# Defining the device being used\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Device available: {device.upper()}\")\n",
        "\n",
        "# Creating the dataset and dataloader\n",
        "audio_dataset = AudioDataset(f\"{LOCAL_IRMAS}.csv\", LOCAL_IRMAS, device, mel_spectogram, SAMPLE_RATE, NUM_SAMPLES)\n",
        "data_loader = DataLoader(audio_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Printing all the class labels\n",
        "print(audio_dataset.get_class_labels())\n",
        "# 0 cel\n",
        "# 1 cla\n",
        "# 2 flu\n",
        "# 3 gac\n",
        "# 4 gel\n",
        "# 5 org\n",
        "# 6 pia\n",
        "# 7 sax\n",
        "# 8 tru\n",
        "# 9 vio\n",
        "# 10 voi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4GdCHUrzH-B"
      },
      "source": [
        "## Convolutional Neural Network\n",
        "\n",
        "Defining the convolutional neural network architecture with the layers and activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coytvGjKgQ9q",
        "outputId": "d06af45c-1add-41b1-f4ea-a5967dbd29f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 66, 46]             160\n",
            "              ReLU-2           [-1, 16, 66, 46]               0\n",
            "         MaxPool2d-3           [-1, 16, 33, 23]               0\n",
            "            Conv2d-4           [-1, 32, 35, 25]           4,640\n",
            "              ReLU-5           [-1, 32, 35, 25]               0\n",
            "         MaxPool2d-6           [-1, 32, 17, 12]               0\n",
            "            Conv2d-7           [-1, 64, 19, 14]          18,496\n",
            "              ReLU-8           [-1, 64, 19, 14]               0\n",
            "         MaxPool2d-9             [-1, 64, 9, 7]               0\n",
            "           Conv2d-10           [-1, 128, 11, 9]          73,856\n",
            "             ReLU-11           [-1, 128, 11, 9]               0\n",
            "        MaxPool2d-12            [-1, 128, 5, 4]               0\n",
            "          Flatten-13                 [-1, 2560]               0\n",
            "           Linear-14                   [-1, 11]          28,171\n",
            "          Softmax-15                   [-1, 11]               0\n",
            "================================================================\n",
            "Total params: 125,323\n",
            "Trainable params: 125,323\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.83\n",
            "Params size (MB): 0.48\n",
            "Estimated Total Size (MB): 2.32\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class CNNNetwork(nn.Module):\n",
        "  ''' Custom Convolution Neural Network for audio classification'''\n",
        "\n",
        "  def __init__(self, num_classes):\n",
        "    '''\n",
        "    Parameters -\n",
        "        num_classes: the number of classes used for classification\n",
        "    '''\n",
        "    super().__init__()\n",
        "\n",
        "    # Define convolutional blocks\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv4 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    # Create a flatten layer to reshape the data into a 1D vector\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    # Create a linear / dense layer to classify the data from the convolutional network\n",
        "    self.linear = nn.Linear(in_features=128*5*4, out_features=num_classes)\n",
        "\n",
        "    # Create a softmax layer to provide decimal probabilities to the class predictions\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, input_data):\n",
        "    ''' Passing the data through the network '''\n",
        "    x = self.conv1(input_data)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear(x)\n",
        "    predictions = self.softmax(logits)\n",
        "    return predictions\n",
        "\n",
        "# Creating the neural network\n",
        "cnn = CNNNetwork(audio_dataset.get_class_labels().size).to(device)\n",
        "\n",
        "# Printing a summary of the network\n",
        "summary(cnn.cuda(), (1, 64, 44))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkebGEMLzwbk"
      },
      "source": [
        "## Training\n",
        "\n",
        "Below demonstrates the function to train the neural network. This then saves the model state to google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfGg8_e1GR8I",
        "outputId": "bf484afa-9925-4126-8180-0a36ac5fc6f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 39:   0%|          | 0/53 [1:03:31<?, ?batch/s, accuracy=38.3, loss=1.54]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Completed\n",
            "Model trained and stored  at '/content/drive/MyDrive/Audio/cnn.pth'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def train(model, data_loader, loss_func, optimiser, device, epochs):\n",
        "  '''\n",
        "  Parameters -\n",
        "    model: the model to train\n",
        "    data_loader: the dataloader to optimise memory storage with batch downloads\n",
        "    loss_func: the loss function to use to update the bias weightings\n",
        "    optimiser: the optimiser to use for backwards propogation\n",
        "    device: the device in use (cuda or cpu)\n",
        "    epochs: the number of epochs to run\n",
        "  '''\n",
        "  model.train()\n",
        "\n",
        "  # Using a loading bar to show progress\n",
        "  with tqdm(data_loader, unit=\"batch\", total=len(data_loader)) as tepoch:\n",
        "\n",
        "    # Increament through each epoch\n",
        "    for epoch in range(epochs):\n",
        "      for input, target in data_loader:\n",
        "        tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        # Load the batch input into the device memory\n",
        "        input, target = input.to(device), target.to(device)\n",
        "\n",
        "        # Calculate the loss\n",
        "        output = model(input) # pass the inputs into the model\n",
        "        loss = loss_func(output, target) # compare the predictions to the actual data\n",
        "\n",
        "        # Calculate the accuracy of the model to display\n",
        "        predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
        "        correct = (predictions == target).sum().item()\n",
        "        accuracy = correct / BATCH_SIZE\n",
        "\n",
        "        # Backpropogate the loss and update the NN weights\n",
        "        optimiser.zero_grad() # resets the gradients\n",
        "        loss.backward() # performs back propogation\n",
        "        optimiser.step() # updates the weights\n",
        "\n",
        "        tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy)\n",
        "\n",
        "  print(\"\\nTraining Completed\")\n",
        "\n",
        "# Defining network constants\n",
        "EPOCHS = 40 # [estimated 1.5 mins / per epoch]\n",
        "LEARNING_RATE = .001\n",
        "\n",
        "# Defining the loss function\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining the optimiser\n",
        "optimiser = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training the model\n",
        "train(cnn, data_loader, loss_func, optimiser, device, EPOCHS)\n",
        "\n",
        "# Saving the model state\n",
        "model_state_path = os.path.join(GDRIVE_AUDIO, \"cnn.pth\")\n",
        "torch.save(cnn.state_dict(), model_state_path)\n",
        "print(f\"Model trained and stored  at '{model_state_path}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn4EoYCKdL98"
      },
      "source": [
        "## References\n",
        "\n",
        "<Bosch2012>Bosch, J. J., Janer, J., Fuhrmann, F., & Herrera, P. (2012). \"A Comparison of Sound Segregation Techniques for Predominant Instrument Recognition in Musical Audio Signals.\" In Proceedings of the International Society for Music Information Retrieval Conference (ISMIR) (pp. 559-564)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
