{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The spectral centroid indicates at which frequency the energy of a spectrum is centred upon\" (Chauhan, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breebaart (2004) compared various features such as mel-frequency cepstral coefficients (MFCC), auditory filter-bank temporal envelope (AFTE), standard low-level (SLL) and psychoacoustic (PA) features when training a standard Gaussian framework for both genre and general audio classification.\n",
    "\n",
    "**AFTE features**: GammaTone filter banks.\n",
    "\n",
    "**SLL features**: Root-mean-square (RMS) level, spectral (centroid, bandwidth, and roll-off), zero-crossing rate, band energy ratio, delta spectrum magnitude, and pitch.\n",
    "\n",
    "**PA features**: Roughness (the perception of temporal envelope modulations in the range of about 20-150 Hz), loudness (the sensation of signal strength) and sharpness (the spectral density and the relative strength of high-frequency energy).\n",
    "\n",
    "It was demonstrated that AFTE features provided the highest accuracy in Breebaart's study with MFCC's coming in at a close second, this corresponds with Liu's (2018) study showing 79% accuracy when training an attention based long short-term memory (LSTM+A) network with GFCC's.\n",
    "\n",
    "**GFCC's** and GammaTone filter bands tend to give a better resolution in the lower frequency ranges and provide a Cochlegram (relatively to Cochlea, a component of the inner ear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The literature above shows the results for general audio classification, whereas Deng (2008) and Racharla et. al (2020) showed that MFCC and perception-based features dominated in SVM's for instrument classification models with up to 79% accuracy.\n",
    "\n",
    "**MFCC's** represent a non-linear 'spectrum-of-a-spectrum' where \"the frequency bands are equally spaced on the mel scale, which approximates the human auditory system’s response\" (Sdour 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rathikarani (2020) shows 98% accuracy with SVM and 95% with kNN classifiers using MFCC features in instrument classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC kNN  best : MLP MFCC & GFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert .bib to Harvard\n",
    "Zaman, K. et al. (2023) ‘A Survey of Audio Classification Using Deep Learning’, IEEE Access, 11, pp. 106620–106649. doi: 10.1109/ACCESS.2023.3318015.\n",
    "\n",
    "\n",
    "Racharla, K. et al. (2020) ‘Predominant Musical Instrument Classification based on Spectral Features’, in 2020 7th International Conference on Signal Processing and Integrated Networks (SPIN). IEEE. doi: 10.1109/spin48934.2020.9071125.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
